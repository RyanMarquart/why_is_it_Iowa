```{r -- loading all libraries}
#libraries
library(ggplot2)
library(GGally)
library(dplyr)
library(readr)
library(MASS)
library(broom)
library(corrr)
library(ggcorrplot)
library(FactoMineR)
library(factoextra)
library(randomForest)
library(tidyverse)
library(tidymodels)
library(glmnet)
library(caret)
library(rpart.plot)
library(vip)
library(gbm)
library(janitor)
```

```{r -- loading data}
#load data -- from GitHub URL
train <- read.csv("https://raw.githubusercontent.com/RyanMarquart/why_is_it_Iowa/main/real_train.csv")
test <- read.csv("https://raw.githubusercontent.com/RyanMarquart/why_is_it_Iowa/main/real_test.csv")
train <- train[-1] %>% mutate_if(is.character,as.factor) 
test <- test[-1] %>% mutate_if(is.character, as.factor)
```

## Data Exploration

```{r - data exploration chunk1}
#data exploration of numerical cols
train_df_k <- data.frame(train)
numerical_cols_train <- select_if(train_df_k, is.numeric)
for (col in names(numerical_cols_train)) {
  plot(numerical_cols_train[,col], train_df_k$SalePrice,xlab=col,ylab="SalePrice")
}
```

```{r - data exploration chunk2}
#data exploration of categorical cols
categorical_cols_train <- select_if(train_df_k, is.factor)
for (col in names(categorical_cols_train)) {
  plot(categorical_cols_train[,col], train_df_k$SalePrice,xlab=col,ylab="SalePrice")
}
```

```{r, message=FALSE}
#graphs for paper: variables with strong relationship to SalePrice

attach(train)
par(mfrow=c(2,2))
plot(GrLivArea,SalePrice, main="Scatterplot of SalePrice vs. GrLivArea")
plot(OverallQual,SalePrice, main="Scatterplot of SalePrice vs OverallQual")
plot(GarageCars,SalePrice, main="Scatterplot of SalePrice vs GarageCars")
plot(X1stFlrSF,SalePrice, main="Scatterplot of SalePrice vs X1stFlrSF")

```


## Data Cleaning
```{r - qqplot of SalePrice/log(SalePrice)}

#This data does not have a normal distribution
ggplot(data  = train, aes(sample = SalePrice)) + stat_qq() + stat_qq_line()


#Log transforming the data makes it appear normally distributed
ggplot(data  = train, aes(sample = log(SalePrice))) + stat_qq() + stat_qq_line()
```

## Stepwise Selection

```{r - fullmodel, fwd/bwd stepwise models}
# Fit the full model 
full.model <- lm(log(SalePrice) ~., data = train)
# Stepwise regression model
#Backward stepwise
step.model <- stepAIC(full.model, direction = "forward", 
                      trace = FALSE)
#Properly naming model
lmForward <- step.model
step.model <- stepAIC(full.model, direction = "backward", 
                      trace = FALSE)
#Properly naming model
lmBackward<- step.model
step.model <- stepAIC(full.model, direction = "both", 
                      trace = FALSE)
#Properly naming model
lmBoth <- step.model
```


```{r, warning=FALSE}
set.seed(478)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.model <- train(SalePrice ~., data = train,
                    method = "leapForward", 
                    tuneGrid = data.frame(nvmax = 1:50),
                    trControl = train.control
                    )
step.model$results

train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.model <- train(SalePrice ~., data = train,
                    method = "leapBackward", 
                    tuneGrid = data.frame(nvmax = 1:50),
                    trControl = train.control
                    )
step.model$results
# 38,221.20 -- Forwards
# 41,765.94 -- Backwards
```


## FAMD

```{r -- setting for FAMD}
#getting data ready for FAMD
train_famd <- train[-c(1,2,79)]
```

```{r -- FAMD on Vars}
#FAMD on all variables (categorical and qualitative)
FAMD(train_famd, ncp = 5, sup.var = NULL, ind.sup = NULL, graph = TRUE)

res_famd <- FAMD(train_famd, graph = FALSE)
eig_val <- get_eigenvalue(res_famd)
fviz_screeplot(res_famd)

var <- get_famd_var(res_famd)

# Plot of variables
fviz_famd_var(res_famd, labelsize = 3, repel = TRUE)

#Importance of variables
fviz_contrib(res_famd, "var", axes = 1) +
    theme(axis.text = element_text(size = 6))
```

## MLR using important variables found with FAMD

```{r}
#mlr with top 15 famd variables
mlr_famd <- lm(log(SalePrice) ~ Neighborhood+OverallQual+YearBuilt+BsmtQual+
               GarageFinish+ExterQual+KitchenQual+Foundation+GarageArea+ 
               GarageType+YearRemodAdd+TotalBsmtSF+Exterior1st+Exterior2nd+
               FullBath, data=train)
summary(mlr_famd)

#rmse
mlr_rmse_famd <- sqrt(mean(train$SalePrice - fitted(mlr_famd))^2)
mlr_rmse_famd
```


## Prepping for Ridge Regression

```{r}
train_log_sp <- real_train %>% mutate(SalePrice = log(SalePrice))
```

```{r}
drop <- ("X")
df1_train = train_log_sp[,!(names(train_log_sp) %in% drop)]
```


```{r}
lm1 <- lm(SalePrice ~ ., data=train_log_sp)
significant <- summary(lm1)$coefficients[, "Pr(>|t|)"] < 0.05
significant[significant == TRUE] # all variables that are significant < 0.05 in basic lm
```


```{r - changing variables}
numericVars <- which(sapply(df1_train, is.numeric)) #index vector numeric variables
factorVars <- which(sapply(df1_train, is.factor)) #index vector factor variables
```

```{r - checking correlation}
train_numVar <- df1_train[, numericVars]
corr_nums <- cor(train_numVar, use="pairwise.complete.obs")

corr_sorted <- as.matrix(sort(corr_nums[,'SalePrice'], decreasing = TRUE))
corr_sorted[corr_sorted > 0.5, ]
```

```{r - changing variable types}
df1_train <- df1_train %>% mutate_if(is.character, as.factor)
# now all characters are factors
df1_train$MoSold <- as.factor(df1_train$MoSold)
```

```{r}
drop <- ("SalePrice")
df1_no_SP = df1_train[,!(names(df1_train) %in% drop)]
```

```{r - variable importance}
set.seed(478)

quick_RF <- randomForest(x=df1_no_SP, y=df1_train$SalePrice, ntree=100,importance=TRUE)

imp_RF <- importance(quick_RF)
imp_DF <- data.frame(Variables = row.names(imp_RF), MSE = imp_RF[,1])
imp_DF <- imp_DF[order(imp_DF$MSE, decreasing = TRUE),]

ggplot(imp_DF[1:20,], aes(x=reorder(Variables, MSE), y=MSE, fill=MSE)) + 
  geom_bar(stat = 'identity') + 
  labs(x = 'Variables', y= 'Variable Importance') + coord_flip() + 
  theme(legend.position="none")
```

## Ridge Regression

```{r -- train/testing data}
set.seed(478)
train_index <- sample(1:nrow(df1_train), size = round(0.7*nrow(df1_train)), replace = FALSE)
ridge_train <- df1_train[train_index,]
ridge_test <- df1_train[-train_index,]
```


```{r - setting x and y}
set.seed(478)
x = model.matrix(SalePrice~., ridge_train)[,-1]

y = ridge_train %>%
  dplyr::select(SalePrice) %>%
  unlist() %>%
  as.numeric()
```

```{r ridge regression}
set.seed(478)
control <- trainControl(method = "cv", number = 10)
ridgeGrid <- expand.grid(alpha = 0, lambda = seq(0.001,0.1,by = 0.0005))

ridge_mod <- train(x=x, y=y, method='glmnet', trControl= control, tuneGrid=ridgeGrid) 

ridge_mod$bestTune
min(ridge_mod$results$RMSE) # 0.1280044 - when using log(SalePrice)
# 27,336.83 -- when not log(SalePrice)
```

```{r}
set.seed(478)
x = model.matrix(SalePrice~., ridge_test)[,-1]

y = ridge_test %>%
  dplyr::select(SalePrice) %>%
  unlist() %>%
  as.numeric()
set.seed(478)
control <- trainControl(method = "cv", number = 10)
ridgeGrid <- expand.grid(alpha = 0, lambda = seq(0.001,0.1,by = 0.0005))

ridge_mod <- train(x=x, y=y, method='glmnet', trControl= control, tuneGrid=ridgeGrid) 

ridge_mod$bestTune
min(ridge_mod$results$RMSE) # 0.1193598 - when using log(SalePrice)
# 25,462.32 -- when not log(SalePrice)

```


```{r, warning=FALSE}
#Ridge Regression Model
set.seed(478)
lambda <- lambda <- 10^seq(-2, 10, length.out = 100)
tune_df <- data.frame(lambda = lambda)
prep_data <- recipe(SalePrice ~ ., data = ridge_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors())

ridge_train_vfold_cv <- vfold_cv(ridge_train, v = 10)

ridge_spec <- linear_reg(mixture = 0, penalty = tune("lambda")) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

workflow() %>%
  add_model(ridge_spec) %>%
  add_recipe(prep_data) %>%
  tune_grid(resamples = ridge_train_vfold_cv, grid = tune_df) -> ridge_tune

ridge_tune %>%
  collect_metrics() %>%
  dplyr::select(lambda, .metric, mean) %>%
  pivot_wider(names_from = .metric, values_from = mean) %>%
  ggplot() +
  geom_line(aes(lambda, rmse^2)) +
  geom_point(aes(lambda, rmse^2)) +
  coord_trans(x = "log10")

show_best(ridge_tune, metric = "rmse", n = 1)
best_lam = show_best(ridge_tune, metric = "rmse", n = 1)[1]$lambda
ridge_spec <- linear_reg(mixture = 0, penalty =(best_lam)) %>%
  set_mode("regression") %>%
  set_engine("glmnet")
  
workflow() %>%
  add_model(ridge_spec) %>%
  add_recipe(prep_data) %>%
  tune_grid(resamples = ridge_train_vfold_cv, grid = tune_df) -> ridge_tune

ridge_tune %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  mutate(mean = mean^2) %>%
  pull(mean)

wf = workflow() %>%
  add_model(ridge_spec) %>%
  add_recipe(prep_data)
best_mod = ridge_tune %>% select_best("rmse")
 
ridge_final_fit = finalize_workflow(wf, best_mod) %>%
  fit(data = ridge_train)
```

```{r, warning=FALSE}
ridge_train_pred = predict(ridge_final_fit, ridge_train)

RMSE(ridge_train_pred[['.pred']], ridge_train$SalePrice) #18,850.29

RMSE(ridge_train_pred[['.pred']], ridge_test$SalePrice) # 106,511.6
```


## Random Forest/Decision Trees Below
```{r}
mean(df1_train$SalePrice)
summary(df1_train$SalePrice)
# Min    - 10.46
# 1st Q  - 11.77
# Median - 12.00
# Mean   - 12.02
# 3rd Q  - 12.27
# Max    - 13.53
```

```{r}
set.seed(478)

quick_RF <- randomForest(x=df1_no_SP, y=df1_train$SalePrice, ntree=500,importance=TRUE)

imp_RF <- importance(quick_RF)
imp_DF <- data.frame(Variables = row.names(imp_RF), MSE = imp_RF[,1])
imp_DF <- imp_DF[order(imp_DF$MSE, decreasing = TRUE),]

ggplot(imp_DF[1:50,], aes(x=reorder(Variables, MSE), y=MSE, fill=MSE)) + 
  geom_bar(stat = 'identity') + 
  labs(x = 'Variables', y= 'MSE') + coord_flip() + 
  theme(legend.position="none")
imp_DF[1:25,] # top 25 most important variables
```

```{r adjusting df1_train for decision trees}
set.seed(478)
df1_factor <- df1_train %>% mutate(price_factor = case_when(
  SalePrice <= 11.77 ~ 'Very Inexpensive',
  SalePrice <= 12.00 ~ 'Inexpensive',
  SalePrice <= 12.15 ~ 'Medium Expensive',
  SalePrice <= 12.20 ~ 'Expensive'))
df1_factor$price_factor <- as.factor(df1_factor$price_factor)
# dropping sale price
drop <- ("SalePrice")
df1_no_sp2 = df1_factor[,!(names(df1_factor) %in% drop)]
```

```{r train/test split for trees}
train_index <- sample(1:nrow(df1_no_sp2), size = round(0.7*nrow(df1_no_sp2)), replace = FALSE)
tree_train <- df1_no_sp2[train_index,]
tree_test <- df1_no_sp2[-train_index,]
```

```{r decision tree for price_factor}
# Example tree - Fit depends on other variables
set.seed(478)
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")
tree_fit <- tree_spec %>%
  fit(price_factor ~ GrLivArea+Neighborhood+OverallQual+TotalBsmtSF+BsmtFinSF1+
        X1stFlrSF+LotArea+GarageArea+X2ndFlrSF+GarageCars+OverallCond+BsmtFinType1+
        FireplaceQu+TotRmsAbvGrd+HouseStyle+GarageType+ExterQual+YearBuilt+KitchenQual+
        Exterior2nd+GarageYrBlt+BsmtUnfSF+OpenPorchSF+Fireplaces+MSZoning, 
      data = df1_no_sp2)

tree_fit 
tree_fit %>%
  augment(new_data = df1_no_sp2) %>%
  accuracy(truth = price_factor, estimate = .pred_class)

tree_test_fit <- tree_spec %>%
  fit(price_factor ~ GrLivArea+Neighborhood+OverallQual+TotalBsmtSF+BsmtFinSF1+
        X1stFlrSF+LotArea+GarageArea+X2ndFlrSF+GarageCars+OverallCond+BsmtFinType1+
        FireplaceQu+TotRmsAbvGrd+HouseStyle+GarageType+ExterQual+YearBuilt+KitchenQual+
        Exterior2nd+GarageYrBlt+BsmtUnfSF+OpenPorchSF+Fireplaces+MSZoning, 
      data = tree_train)

tree_test_fit %>%
  augment(new_data = tree_test) %>%
  accuracy(truth = price_factor, estimate = .pred_class)

tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot()

tree_test_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```


## LASSO

```{r}
test_real <- read_csv("real_test.csv")
train_real <- read_csv("real_train.csv")

train_real <- as.data.frame(unclass(train_real), stringsAsFactors = TRUE)
test_real <- as.data.frame(unclass(train_real), stringsAsFactors = TRUE)

set.seed(478)
#Make y value sales price
y <- train_real$SalePrice

#Sets x to all other variables for predictors
x <- data.matrix(train_real[,!names(train_real) %in% c("SalePrice", "TBsmtFullBath")])

#Perform k-fold cross-validation
cv_model <- cv.glmnet(x, y, alpha = 1)

#Find best lambda value 
best_lambda <- cv_model$lambda.min
best_lambda

#Make plot of test MSE by lambda value
plot(cv_model) 

```


```{r}
#Makes the best lasso model and the coefficients from it
best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(best_model)
```


```{r}
#Makes values to test the lasso model against
test_x <- data.matrix(test_real)
#test_y <- test_real$SalePrice

#Matrix of estimated values from lasso model
prediction_lasso <- predict(best_model, s = best_lambda, newx = test_x)
```


## Gradient Boosting

```{r}
#Make gradient boosting model
temp_boost <- gbm(SalePrice ~ . ,data = train_real, distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)

#Shows model variables
summary(temp_boost)
```

```{r}
#Makes vectors for the number of trees being created
num_trees = seq(from=100 ,to=10000, by=100) 

#Makes a prediction matrix for each tree
prediction_matrix <- predict(temp_boost, test_real, n.trees = num_trees)

#Calculates MSE
gradient_error<-with(train_real,apply( (prediction_matrix-SalePrice)^2,2,mean))
summary(gradient_error) 

#Head and tails of the test error data frame to find where MSE is lowest
head(gradient_error)
tail(gradient_error)
Gradient_MSE <- unname(gradient_error[100])
#MSE is lowest for last variable

```


```{r}
#Make data frames for the predicted values
colnames(prediction_matrix)[100] <- "Grad_Estimates"
Grad_estimates <- data.frame(prediction_matrix[,"Grad_Estimates"])

Lasso_estimates <- prediction_lasso
```

## LASSO and Gradient model results

```{r}
#LASSO RMSE
Test_LASSO_RMSE <- sqrt(mean((test_real$SalePrice - Lasso_estimates)^2))
Test_LASSO_RMSE


#Gradient RMSE
Test_Gradient_RMSE <- sqrt(mean((test_real$SalePrice - Grad_estimates$prediction_matrix....Grad_Estimates..)^2))
Test_Gradient_RMSE
```

