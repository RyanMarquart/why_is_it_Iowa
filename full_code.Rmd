#libraries
library(ggplot2)
library(GGally)
library(dplyr)
library(readr)
library(MASS)
library(broom)
library(corrr)
library(ggcorrplot)
library(FactoMineR)
library(factoextra)
library(randomForest)
library(tidyverse)
library(tidymodels)
library(glmnet)
library(caret)
library(rpart.plot)
library(vip)


#load data
train <- read_csv("real_train.csv")
test <- read_csv("real_test.csv")

train <- train[-1] %>% mutate_if(is.character,as.factor) 
test <- test[-1] %>% mutate_if(is.character, as.factor)



# FULL EVAN CODE - UN-EDITED --> STILL NEED TO FIX/INTEGRATE

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r setup2, include=FALSE}
# libraries
library(randomForest)
library(tidyverse) 
library(tidymodels)
library(glmnet)
library(caret)
library(rpart.plot)
library(vip)
# library(knitr)
# library(reshape2)
library(corrplot)
# library(gridExtra)
# library(scales)
```

```{r reading in train/test data, include=FALSE}
real_test <- read.csv("Desktop/real_test.csv")
real_train <- read.csv("Desktop/real_train.csv")
```

## My Tasks:
Random Forest
Ridge Regression

```{r}
real_train <- real_train %>% mutate(SalePrice = log(SalePrice))
```

```{r}
drop <- ("X")
df1_train = real_train[,!(names(real_train) %in% drop)]
df1_test = real_test[,!(names(real_test) %in% drop)]
```


```{r}
lm1 <- lm(SalePrice ~ ., data=df1_train)
significant <- summary(lm1)$coefficients[, "Pr(>|t|)"] < 0.05
significant[significant == TRUE] # all variables that are significant < 0.05 in basic lm
```


```{r - changing variables}
numericVars <- which(sapply(df1_train, is.numeric)) #index vector numeric variables
factorVars <- which(sapply(df1_train, is.factor)) #index vector factor variables
```

```{r - checking correlation}
train_numVar <- df1_train[, numericVars]
corr_nums <- cor(train_numVar, use="pairwise.complete.obs")

corr_sorted <- as.matrix(sort(corr_nums[,'SalePrice'], decreasing = TRUE))
corr_sorted[corr_sorted > 0.5, ]
```

```{r - changing variable types}
df1_train <- df1_train %>% mutate_if(is.character, as.factor)
# now all characters are factors
df1_train$MoSold <- as.factor(df1_train$MoSold)
```

```{r}
drop <- ("SalePrice")
df1_no_SP = df1_train[,!(names(df1_train) %in% drop)]
```

```{r - variable importance}
set.seed(478)

quick_RF <- randomForest(x=df1_no_SP, y=df1_train$SalePrice, ntree=100,importance=TRUE)

imp_RF <- importance(quick_RF)
imp_DF <- data.frame(Variables = row.names(imp_RF), MSE = imp_RF[,1])
imp_DF <- imp_DF[order(imp_DF$MSE, decreasing = TRUE),]

ggplot(imp_DF[1:20,], aes(x=reorder(Variables, MSE), y=MSE, fill=MSE)) + 
  geom_bar(stat = 'identity') + 
  labs(x = 'Variables', y= 'Variable Importance') + coord_flip() + 
  theme(legend.position="none")

```

# Beginning of Ridge Regression
```{r - setting x and y}
set.seed(478)
x = model.matrix(SalePrice~., df1_train)[,-1]

y = df1_train %>%
  dplyr::select(SalePrice) %>%
  unlist() %>%
  as.numeric()
```

```{r ridge regression}
set.seed(478)
control <- trainControl(method = "cv", number = 10)
ridgeGrid <- expand.grid(alpha = 0, lambda = seq(0.001,0.1,by = 0.0005))

ridge_mod <- train(x=x, y=y, method='glmnet', trControl= control, tuneGrid=ridgeGrid) 

ridge_mod$bestTune
min(ridge_mod$results$RMSE) # 0.1185774
```

```{r -- train/testing data}
set.seed(478)
train_index <- sample(1:nrow(df1_train), size = round(0.7*nrow(df1_train)), replace = FALSE)
train <- df1_train[train_index,]
test <- df1_train[-train_index,]
```

```{r, warning=FALSE}
#Ridge Regression Model
set.seed(478)
lambda <- lambda <- 10^seq(-2, 10, length.out = 100)
tune_df <- data.frame(lambda = lambda)
prep_data <- recipe(SalePrice ~ ., data = train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors())

df1_train_vfold_cv <- vfold_cv(train, v = 10)

ridge_spec <- linear_reg(mixture = 0, penalty = tune("lambda")) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

workflow() %>%
  add_model(ridge_spec) %>%
  add_recipe(prep_data) %>%
  tune_grid(resamples = df1_train_vfold_cv, grid = tune_df) -> ridge_tune

ridge_tune %>%
  collect_metrics() %>%
  dplyr::select(lambda, .metric, mean) %>%
  pivot_wider(names_from = .metric, values_from = mean) %>%
  ggplot() +
  geom_line(aes(lambda, rmse^2)) +
  geom_point(aes(lambda, rmse^2)) +
  coord_trans(x = "log10")

show_best(ridge_tune, metric = "rmse", n = 1)
best_lam = show_best(ridge_tune, metric = "rmse", n = 1)[1]$lambda
ridge_spec <- linear_reg(mixture = 0, penalty =(best_lam)) %>%
  set_mode("regression") %>%
  set_engine("glmnet")
  
workflow() %>%
  add_model(ridge_spec) %>%
  add_recipe(prep_data) %>%
  tune_grid(resamples = df1_train_vfold_cv, grid = tune_df) -> ridge_tune

ridge_tune %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  mutate(mean = mean^2) %>%
  pull(mean)

wf = workflow() %>%
  add_model(ridge_spec) %>%
  add_recipe(prep_data)
best_mod = ridge_tune %>% select_best("rmse")

ridge_final_fit = finalize_workflow(wf, best_mod) %>%
  fit(data=train)
```

```{r, warning=FALSE}
ridge_train_pred = predict(ridge_final_fit, train)

RMSE(ridge_train_pred[['.pred']], train$SalePrice) #0.09483243

RMSE(ridge_train_pred[['.pred']], test$SalePrice) # 0.5496015
```



## Random Forest/Decision Trees Below
```{r}
mean(df1_train$SalePrice)
summary(df1_train$SalePrice)
# Min    - 10.46
# 1st Q  - 11.77
# Median - 12.00
# Mean   - 12.02
# 3rd Q  - 12.27
# Max    - 13.53

```

```{r}
set.seed(478)

quick_RF <- randomForest(x=df1_no_SP, y=df1_train$SalePrice, ntree=500,importance=TRUE)

imp_RF <- importance(quick_RF)
imp_DF <- data.frame(Variables = row.names(imp_RF), MSE = imp_RF[,1])
imp_DF <- imp_DF[order(imp_DF$MSE, decreasing = TRUE),]

ggplot(imp_DF[1:50,], aes(x=reorder(Variables, MSE), y=MSE, fill=MSE)) + 
  geom_bar(stat = 'identity') + 
  labs(x = 'Variables', y= 'Variable Importance') + coord_flip() + 
  theme(legend.position="none")
imp_DF[1:25,] # top 25 most important variables
```


```{r adjusting df1_train for decision trees}
set.seed(478)
df1_factor <- df1_train %>% mutate(price_factor = case_when(
  SalePrice <= 10.46 ~ 'Very Inexpensive',
  SalePrice <= 11.77 ~ 'Inexpensive',
  SalePrice <= 12.00 ~ 'Medium Expensive',
  SalePrice <= 12.02 ~ 'Expensive',
  SalePrice >= 12.27 ~ 'Very Expensive'))
df1_factor$price_factor <- as.factor(df1_factor$price_factor)
head(df1_factor)
```

```{r}
drop <- ("SalePrice")
df1_no_SP = df1_factor[,!(names(df1_train) %in% drop)]
```

```{r decision tree for PPO}
# Example tree - Fit depends on other variables
set.seed(478)
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")
tree_fit <- tree_spec %>%
  fit(price_factor ~ ., 
      data = df1_no_SP)

tree_fit 
tree_fit %>%
  augment(new_data = df1_no_SP) %>%
  accuracy(truth = price_factor, estimate = .pred_class)

tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

# END OF EVAN FULL CODE


























